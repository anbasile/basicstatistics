#+TITLE: Basic statistics
#+AUTHOR: Angelo Basile
#+EMAIL: me@angelobasile.it

* Practical 1
Firs we begin be importing our dataset. We have to use an additional library to read an *.xlsx file. After that we assign names to the columns. And we take a look at the first rows of the dataset to see what it looks like.

#+BEGIN_SRC R :session :exports both
data <- read.csv("./data/p1.csv")
names(data) <- c("id","age","sex","profs")
head(data)
#+END_SRC

#+RESULTS:
| 1 | 16 | 1 | 91 |
| 2 | 20 | 2 | 58 |
| 3 | 24 | 1 | 52 |
| 4 | 22 | 2 | 45 |
| 5 | 18 | 1 | 78 |
| 6 | 14 | 2 | 88 |

** Question 1
/Does the proficiency score increase or decrease with age?/

To answer this question we might simply plot both age and proficiency score and see if there is any clear indication.

#+name: fig1
#+BEGIN_SRC R :session :file 1.png :results value graphics :exports both
plot(data$age,data$profs)
#+END_SRC

#+RESULTS: fig1
[[file:2.png]]

Well, yes, from the scatter plot we built we can see that as the age increases the proficiency decreases.

** Question 2
/Is there a difference in proficiency score between female and male participants?/

#+BEGIN_SRC R :session :exports both 
aggregate(data$profs, by=list(Category=data$sex), FUN=mean)
#+END_SRC

#+RESULTS:
| 1 | 75.5333333333333 |
| 2 | 63.6666666666667 |

** Question 3
/Is there an overall difference in age between male and female participants?/

#+BEGIN_SRC R :session :file 2.png :results value graphics
m <- data$age[data$sex == 1]
f <- data$age[data$sex == 2]
boxplot(m,f)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :session :exports both
aggregate(data$age, by=list(Category=data$sex), FUN=mean)
#+END_SRC

#+RESULTS:
| 1 | 20.1333333333333 |
| 2 | 20.5333333333333 |

* Practical 2

#+BEGIN_SRC R :session :exports both
data <- read.csv("./data/p1.csv")
names(data) <- c("id","age","sex","profs")
str(data)
#+END_SRC

#+RESULTS:

** Which age occurs most often?
#+BEGIN_SRC R :session :exports both 
which.max(tabulate(data$age))
#+END_SRC

#+RESULTS:
: 16

** Find out what’s the mean, the median, the mode, the range and the standard deviation of the Proficiency Score in your data.

#+BEGIN_SRC R :session :exports both 
mean(data$profs)
#+END_SRC

#+RESULTS:
: 69.6

#+BEGIN_SRC R :session :exports both
median(data$profs)
#+END_SRC

#+RESULTS:
: 69

#+BEGIN_SRC R :session :exports both
which.max(tabulate(data$profs))
#+END_SRC

#+RESULTS:
: 59

#+BEGIN_SRC R :session :exports both
range(data$profs)
#+END_SRC

#+RESULTS:
| 33 |
| 97 |

#+BEGIN_SRC R :session :exports both
sd(data$profs)
#+END_SRC

#+RESULTS:
: 16.4434244273069


** Find out what is the minimum age, the maximum age, the mean age and the standard deviation.
#+BEGIN_SRC R :session :exports both
min(data$age)
#+END_SRC

#+RESULTS:
: 14

#+BEGIN_SRC R :session :exports both
max(data$age)
#+END_SRC

#+RESULTS:
: 27

#+BEGIN_SRC R :session :exports both
mean(data$age)
#+END_SRC

#+RESULTS:
: 20.3333333333333

#+BEGIN_SRC R :session :exports both
sd(data$age)
#+END_SRC

#+RESULTS:
: 3.56547944576335

#+BEGIN_SRC R :session :exports both
sd(data$age)
#+END_SRC

#+RESULTS:
: 3.56547944576335

** What is the most frequently occurring proficiency score
#+BEGIN_SRC R :session :exports both
which.max(tabulate(data$profs))
#+END_SRC

#+RESULTS:
: 59

** What is the z-score of Participant 13?
#+BEGIN_SRC R :session :exports both 
scale(data$profs,center=TRUE, scale=TRUE)[13]
#+END_SRC

#+RESULTS:


** Which group has higher proficiency scores, the male or the female participants?
#+BEGIN_SRC R :session :exports both 
aggregate(data$profs, by=list(Category=data$sex), FUN=mean)
#+END_SRC

#+RESULTS:
| 1 | 75.5333333333333 |
| 2 | 63.6666666666667 |

** Which group scored more homogeneously?

#+BEGIN_SRC R :session :exports both 
aggregate(data$profs, by=list(Category=data$sex), FUN=sd)
#+END_SRC

#+RESULTS:
| 1 | 14.2421239721502 |
| 2 | 16.7871833197092 |
** Boxplots
#+BEGIN_SRC R :session :file 3.png :results value graphics :exports both
m <- data$profs[data$sex == 1]
f <- data$profs[data$sex == 2]
boxplot(m,f,names=c("M","F"))
#+END_SRC

#+RESULTS:
[[file:3.png]]

** Part B
Provide the mean, the mode, the median, the range and the standard deviation.
#+BEGIN_SRC R :session :exports both
a <- c(3, 4, 5, 6, 7, 8, 9)
b <- c(6, 6, 6, 6, 6, 6, 6)
c <- c(4, 4, 4, 6, 7, 7, 10)
d <- c(1, 1, 1, 4, 9, 12, 14)
#+END_SRC

#+BEGIN_SRC R :session :exports both
MySummary <- function(dataset) {
  m = mean(dataset)
  mode = which.max(tabulate(dataset))
  med = median(dataset)
  stdde = sd(dataset)
  results <- c(m,mode,med,stdde)
  return(results)
}

#+END_SRC

#+BEGIN_SRC R :session :exports both
MySummary(a)
#+END_SRC

#+BEGIN_SRC R :session :exports both
MySummary(b)
#+END_SRC

#+BEGIN_SRC R :session :exports both
MySummary(c)
#+END_SRC

#+BEGIN_SRC R :session :exports both
MySummary(d)
#+END_SRC
* Practical 3
** Part A

As always, we begin by importing the data and taking a quick look at the first rows to see what it looks like.
#+BEGIN_SRC R :session :exports both
data <- read.csv("./data/p3a.csv",na="",header=TRUE)
head(data)
#+END_SRC

#+RESULTS:
| 1 | A | 1A | 10 | 5 | 5 | 7 | 4 | 2 | 4 | 14 | 5 | 5 | 5 | 0 | 5 | 0 | 0 | 4 | 12 |
| 2 | A | 1A | 12 | 5 | 4 | 8 | 4 | 4 | 5 | 18 | 5 | 0 | 5 | 0 | 0 | 5 | 0 | 4 | 17 |
| 3 | A | 1A | 10 | 4 | 5 | 6 | 2 | 3 | 0 |  8 | 0 | 5 | 5 | 0 | 5 | 0 | 0 | 4 | 11 |
| 4 | A | 1A | 18 | 5 | 6 | 8 | 5 | 3 | 4 | 15 | 5 | 5 | 5 | 0 | 5 | 0 | 5 | 4 | 12 |
| 5 | A | 1B | 20 | 5 | 6 | 7 | 5 | 4 | 4 | 19 | 5 | 5 | 5 | 0 | 5 | 0 | 0 | 5 | 13 |
| 6 | A | 1A | 16 | 5 | 6 | 8 | 6 | 3 | 1 | 19 | 0 | 0 | 5 | 0 | 5 | 0 | 0 | 4 | 11 |

Now we define the type of variables for =teacher= and =group=. More precisely, we want to define them as /factors/.
#+BEGIN_SRC R :session :exports both :results output
data$group <- as.factor(data$group)
data$teacher <- as.factor(data$teacher)
str(data)
#+END_SRC

#+RESULTS:
#+begin_example
Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	130 obs. of  20 variables:
 $ Student#: chr  "1" "2" "3" "4" ...
 $ teacher : Factor w/ 2 levels "A","B": 1 1 1 1 1 1 1 1 1 1 ...
 $ group   : Factor w/ 5 levels "1A","1B","1C",..: 1 1 1 1 2 1 1 1 1 1 ...
 $ Q1      : num  10 12 10 18 20 16 10 7 20 11 ...
 $ Q2      : num  5 5 4 5 5 5 3 4 4 5 ...
 $ Q3      : num  5 4 5 6 6 6 4 6 6 5 ...
 $ Q4      : num  7 8 6 8 7 8 8 6 6 7 ...
 $ Q5      : num  4 4 2 5 5 6 5 3 6 6 ...
 $ Q6      : num  2 4 3 3 4 3 3 2 3 3 ...
 $ Q7      : num  4 5 0 4 4 1 3 0 4 2 ...
 $ Q8      : num  14 18 8 15 19 19 16 14 17 17 ...
 $ Q9      : num  5 5 0 5 5 0 0 0 5 5 ...
 $ Q10     : num  5 0 5 5 5 0 5 0 5 5 ...
 $ Q11     : num  5 5 5 5 5 5 5 5 5 5 ...
 $ Q12     : num  0 0 0 0 0 0 5 0 0 0 ...
 $ Q13     : num  5 0 5 5 5 5 5 0 5 0 ...
 $ Q14     : num  0 5 0 0 0 0 0 0 0 0 ...
 $ Q15     : num  0 0 0 5 0 0 0 0 5 0 ...
 $ Q16     : num  4 4 4 4 5 4 2 5 4 4 ...
 $ Q17     : num  12 17 11 12 13 11 11 2 8 12 ...
#+end_example

*** Descriptives and graphs for groups
Adding a =TOTAL_score= variable.
#+BEGIN_SRC R :session :exports both :results output
data$TOTAL_score <- rowSums(data[,4:20])
str(data$TOTAL_score)
#+END_SRC

#+RESULTS:
:  num [1:130] 87 96 68 105 108 89 85 54 103 87 ...

*** Which performed best? And which group performed most homogeneously?
#+BEGIN_SRC R :session :exports both
best <- aggregate(data$TOTAL_score, by=list(data$group), FUN=mean)
best$Group.1[which.max(best$x)]
#+END_SRC  

#+RESULTS:
: 1B

#+BEGIN_SRC R :session :exports both
more_homo <- aggregate(data$TOTAL_score, by=list(data$group), FUN=sd)
more_homo$Group.1[which.min(more_homo$x)]
#+END_SRC

#+RESULTS:
: 1C

*** Which teacher performed best?
#+BEGIN_SRC R :session :exports both
byteacher <- aggregate(data$TOTAL_score, by=list(data$teacher), FUN=mean)
#+END_SRC

#+RESULTS:
| A |             78.4 |
| B | 69.2928571428571 |

Teacher A

*** Boxplot
#+BEGIN_SRC R :session :file teacher.png :results value graphics :exports both
teacherA <- data$TOTAL_score[data$teacher == "A"]
teacherB <- data$TOTAL_score[data$teacher == "B"]
boxplot(teacherA,teacherB)
#+END_SRC

#+RESULTS:
[[file:teacher.png]]

*** Grades
#+BEGIN_SRC R :session :exports both :results output
data$grade <- trunc(((data$TOTAL_score/143)*100)/10)
str(data$grade)
#+END_SRC

#+RESULTS:
:  num [1:130] 6 6 4 7 7 6 5 3 7 6 ...

*** How many students passed?
#+BEGIN_SRC R :session :exports both
table(data$grade >= 6)
#+END_SRC

#+RESULTS:
| FALSE | 86 |
| TRUE  | 44 |

*** Checking for normality

#+BEGIN_SRC R :session :file normality.png :results value graphics :exports both
x <- data$grade
h<-hist(x, breaks=10, col="red", xlab="Grade", main="Histogram with normal curve of grades")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2) 
#+END_SRC

#+RESULTS:
[[file:normality.png]]

*** Zscores
#+BEGIN_SRC R :session :exports both
zgrades <- scale(data$grade,center=TRUE, scale=TRUE)
round(zgrades[c(11,33,44,55)],2)
#+END_SRC

#+RESULTS:
|   0.8 |
| -1.58 |
|  1.99 |
|   0.8 |

*** Impressions about teacher gorup
It seems to me that teacher A is a better one.
*** The Null hypothesis
/There is no difference between the two groups./
*** Defining the variables
=teacher= is the independent variable.

*** Running the test
The default R's function assumes that there is non equal variance between the two groups. So we first check if that's the case, and in case the variance is equal, then we pass an additional argument to the function.

#+BEGIN_SRC R :session :exports both
var(teacherA)
#+END_SRC

#+RESULTS:
: 481.871186440678

#+BEGIN_SRC R :session :exports both
var(teacherB)
#+END_SRC

#+RESULTS:
: 554.467339544513


Well, at this point I don't know if this difference can be considered large enough to justify the use of the Welsh test. I'll run both.
#+BEGIN_SRC R :session :exports both
t.test(teacherA,teacherB, var.equal=TRUE)$p.value
#+END_SRC

#+RESULTS:
: 0.0250141709914793

#+BEGIN_SRC R :session :exports both
t.test(teacherA,teacherB)$p.value
#+END_SRC


#+RESULTS:
: 0.02426194067448

It is safe to reject the null hypothesis.

** Part B
#+BEGIN_SRC R :session :exports both :results output
data <- read.csv("./data/p3b.csv",na="")
colnames(data) <- c("partecipant","motivation","score")
str(data)
#+END_SRC

#+RESULTS:
: 'data.frame':	424 obs. of  3 variables:
:  $ partecipant: int  1 2 3 4 5 6 7 8 9 10 ...
:  $ motivation : Factor w/ 2 levels "High","Low": 2 2 1 2 2 2 2 1 1 1 ...
:  $ score      : int  22 28 28 26 18 31 22 25 20 25 ...

Let's define Motivation as factor.
#+BEGIN_SRC R :session :exports both :results output
data$motivation <- as.factor(data$motivation)
str(data$motivation)
#+END_SRC

#+RESULTS:
:  Factor w/ 2 levels "High","Low": 2 2 1 2 2 2 2 1 1 1 ...

Ok, now we group the scores by motivation level.
#+BEGIN_SRC R :session :exports both
bymotivation <- aggregate(data$score, by=list(data$motivation), FUN=mean)
#+END_SRC

#+RESULTS:
| High | 23.8842592592593 |
| Low  | 22.8653846153846 |

Good. There is a difference. Now we have to understand if this difference is significative or not.

#+BEGIN_SRC R :session :exports both :results output
low <- data$score[data$motivation == "Low"]
high <- data$score[data$motivation == "High"]
t.test(low,high)
#+END_SRC

#+RESULTS:
#+begin_example

	Welch Two Sample t-test

data:  low and high
t = -2.0046, df = 421.24, p-value = 0.04565
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.01795621 -0.01979307
sample estimates:
mean of x mean of y 
 22.86538  23.88426
#+end_example

#+BEGIN_SRC R :session :exports both
round(t.test(low,high)$p.value,digits=3)
#+END_SRC

#+RESULTS:
: 0.046

Yes, with this /p/ value the difference can be considered significative.
* Practical 4

Inductive statistics

** Applying the t-test

A researcher wants to find out whether boys or girls are more intelligent. Eleven girls and eight boys (randomly selected) participated in an experiment in which scores were involved ranging 1-20 (interval).

| Girls | Boys |
|-------+------|
|    17 |   16 |
|    16 |   15 |
|    14 |   13 |
|    19 |   19 |
|    18 |   15 |
|    17 |   14 |
|    16 |   13 |
|    15 |   12 |
|    16 |      |
|    15 |      |
|    19 |      |

We begin by building the dataframe.

#+BEGIN_SRC R :session :exports both
partecipant <- seq(1,19)
score <- c(17,16,16,15,14,13,19,19,18,15,17,14,16,13,15,12,16,15,19)
gender <- c(1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,1,1)
gender <- as.factor(gender)
levels(gender) <- c("F", "M")
df = data.frame(partecipant,gender,score)
str(df)
#+END_SRC

#+RESULTS:

Here we load some libraries that we are going to use later on. The first one is a plotting library, while the second contains skewness and kurtosis functions. The car packages contains Levene's test.
#+BEGIN_SRC R :session
library(psych)
library(ggplot2)
library(moments)
library(lawstat)
#+END_SRC

#+RESULTS:
| psych     |
| car       |
| moments   |
| ggplot2   |
| ltm       |
| polycor   |
| msm       |
| MASS      |
| stats     |
| graphics  |
| grDevices |
| utils     |
| datasets  |
| methods   |
| base      |

*** What are the dependent and independent variables?
Gender is the independent variable and the score is the dependent one
*** What kind of measures (nominal, ordinal or interval / scale) are used for the variables?
Gender is a nominal, while score is a scale variable.  
*** How many levels does the independent variable have?
Two, =boys= and =girls=. For readability in the output I have renamed these to =M= and =F= respectively.
*** Formulate the statistical hypothesis
- Null: there is no difference in the two groups
- H1: there is a difference: boys do better than girls
- H2: there is a difference: girls do better than boys
*** Select an alpha level suitable for this study  
0.5
*** Which statistical test could be used ?
The t-test. But we have first to check for the normality of the distribution and the homogenity.
  
*** Enter the data
/Tip/: carefully consider this step – the two columns (Girls and Boys) in the data are not necessarily the variable columns. Remember that the columns in the dataset represent variables, not levels of variables!
#+BEGIN_SRC R :session :exports both
head(df)
#+END_SRC

#+RESULTS:
| 1 | F | 17 |
| 2 | M | 16 |
| 3 | F | 16 |
| 4 | M | 15 |
| 5 | F | 14 |
| 6 | M | 13 |

*** Provide the following descriptive statistics for both groups: means, range, minimum, maximum, standard deviations.
#+BEGIN_SRC R :session :exports both :results output
f <- df$score[df$gender == "F"]
m <- df$score[df$gender == "M"]
summary(m)
summary(f)
mean(m)
mean(f)
range(m)
range(f)
sd(m)
sd(f)
#+END_SRC

#+RESULTS:
#+begin_example
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  12.00   13.00   14.50   14.62   15.25   19.00
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  14.00   15.50   16.00   16.55   17.50   19.00
[1] 14.625
[1] 16.54545
[1] 12 19
[1] 14 19
[1] 2.199838
[1] 1.634848
#+end_example

*** What are your first impressions about the difference between the boys and the girls?
Let's take a look.

#+BEGIN_SRC R :session :file boysgirls.png :results value graphics :exports both
boxplot(f,m,names=c("Girls","Boys"))
#+END_SRC

#+RESULTS:
[[file:boysgirls.png]]

It seems that girls score better than the boys.
*** Create a box plot to visualise the results. 
Done in the previous section.
*** Test the statistical significance of this experiment

Find out which group has a distribution that most resembles the normal distribution.

  What do the values of skewness and kurtosis represent again ?  How can they help you in determining whether a dataset resembles a normal distribution?  Check the “How To Check Assumptions NEW” on Nestor as well.

In our dataset we have 19 observations. So, we are going to run the Shapiro-Wilk test.

#+BEGIN_SRC R :session :exports both :results output
shapiro.test(m)
shapiro.test(f)
#+END_SRC

#+RESULTS:
#+begin_example

	Shapiro-Wilk normality test

data:  m
W = 0.9228, p-value = 0.453

	Shapiro-Wilk normality test

data:  f
W = 0.94182, p-value = 0.5422
#+end_example

Both groups resembles a normal distribution. We now take a look at skewness and kurtosis.

#+BEGIN_SRC R :session :exports both :results output
skewness(m)
kurtosis(m)
skewness(f)
kurtosis(f)
#+END_SRC

#+RESULTS:
: [1] 0.8540259
: [1] 3.008633
: [1] 0.203529
: [1] 2.014369

The =boys= group presents higher values for both skewness and kurtosis when compaird to =girls=. So =girls= has a more normal distribution.

**** Do  the Independent samples t-test.
Why do you have to use this test rather than the one sample t-test or the paired samples t-test ?

#+BEGIN_SRC R :session :exports both :results output
t.test(m,f)
#+END_SRC

#+RESULTS:
#+begin_example

	Welch Two Sample t-test

data:  m and f
t = -2.0856, df = 12.357, p-value = 0.05838
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -3.92030824  0.07939915
sample estimates:
mean of x mean of y 
 14.62500  16.54545
#+end_example

**** Carefully study the output
**** Leven's test
Taking Levene’s test into account, what is the value of “t”?  Which degrees of freedom are applied to this test?  What is the level of significance of these samples ?  Compare this to the alpha level you set in e) above.  Can you reject H 0 ?
#+RESULTS:

#+BEGIN_SRC R :session :exports both :results output
levene.test(df$score, df$gender, location="median")
#+END_SRC

#+RESULTS:
: 
: 	modified robust Brown-Forsythe Levene-type test based on the absolute
: 	deviations from the median
: 
: data:  df$score
: Test Statistic = 0.38995, p-value = 0.5406

Since the p-value of the Levene's test is greater than 0.05, I would say that the test is not signicant and so the two groups should have a similar variance. But from the plot it doesn't seem so. Indeed if we compare the two variances we can see that one is more than twice the other. I suspect there is something wrong with the test.

#+BEGIN_SRC R :session :exports both :results output
var(m)
var(f)
#+END_SRC

#+RESULTS:
: [1] 4.839286
: [1] 2.672727


#+BEGIN_QUOTE
On average, the girls showed a higher level of intelligence (M=14.63, SE= ... )  than the boys(M=14.63. , SE= ... ). This difference was not significant t(df=12.36,t=-2.09, p > 0.05).
#+END_QUOTE

** What can you say about the meaningfulness of this outcome?
Is there any additional information you’d like to have about this study ?

Not much. I would like to have more data
** Consider the following data
8 students have participated in a reading test and a listening comprehension test.  Reading ability and listening comprehension are operationalised by the variables R and L respectively. Both variables are measured on an interval scale. The results have been summarised in the table below. Build a dataframe.

| Student |   R |  L |
|---------+-----+----|
|       1 |  20 | 65 |
|       2 |  40 | 69 |
|       3 |  60 | 73 |
|       4 |  80 | 77 |
|       5 | 100 | 80 |
|       6 | 120 | 84 |
|       7 | 140 | 89 |
|       8 | 160 | 95 |
#+BEGIN_SRC R :session
partecipant <- seq(1,8)
r <- c(20,40,60,80,100,120,140,160)
l <- c(65,69,73,77,80,84,89,95)
df = data.frame(partecipant,r,l)
#+END_SRC

#+RESULTS:
| 1 |  20 | 65 |
| 2 |  40 | 69 |
| 3 |  60 | 73 |
| 4 |  80 | 77 |
| 5 | 100 | 80 |
| 6 | 120 | 84 |
| 7 | 140 | 89 |
| 8 | 160 | 95 |

*** What would be H_0 if we want to test the relationship between reading and listening comprehension?
Reading and listening do not interfere.  
*** Make a plot of the results.

#+BEGIN_SRC R :session :file correlation.png :results value graphics :exports both
plot(df$r,df$l,xlab="Reading",ylab="Listening")
#+END_SRC

*** At face value, do you think Reading and Listening , as plotted in the graph, are related?
Yes
*** We want to know if we can conclude that reading skills and listening comprehension are significantly related.
To determine this, you will have to calc ulate a Pearson r (or r xy ). Make sure the computer calculates the Pearson correlation for a two-tailed test.  What is the value of r xy ? Is this a strong correlation? What is the chance of incorrectly rejecting your H 0 ? What do you decide?

#+BEGIN_SRC R :session :exports both :results output
cor(df$r,df$l,method="pearson")
#+END_SRC

#+RESULTS:
: 0.996229128491916

#+BEGIN_SRC R :session :exports both :results output
t.test(df$r,df$l)
#+END_SRC

#+RESULTS:
#+begin_example

	Welch Two Sample t-test

data:  df$r and df$l
t = 0.62193, df = 7.5972, p-value = 0.5522
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -30.16518  52.16518
sample estimates:
mean of x mean of y 
       90        79
#+end_example

*** Report

#+BEGIN_QUOTE
A correlation analysis showed that Reading Skills and Listening Skills were not significantly related (r =0.99, p > 0.05)
#+END_QUOTE
*** Cronbach's Alpha

...we shortly discussed reliability, and that Cronbach’s Alpha was a good measure to check for reliability of a test. The teachers from the data in Practical 3A are interested in the reliability of their exam. They have decided to use Cronbach’s Alpha to check this
1) Open the data for Prac3A t o check the reliability of a 17-item phonetics test
2) Decide whether the test is reliable by going to Analyze > Scale > Reliability Analysis.  Put all the Qu estions in the Items (and not the Total and the Grade), and choose Alpha next to Model . Click OK. The Output will give you a correlation coefficient.  Do you think this is a reliable test?
3) Now we will check the individual items. Go to Analyze > Scale > Reliability Analysis.  Click on Statistics. Check Inter-Item Correlations and Descriptives for Scale if item deleted. Click OK. The output will give you the correlations between items and will give you all the Cronbach’s Alpha values without a particular item. With the deletion of which item do you get the highest reliabil

#+BEGIN_SRC R :session :exports both :results output
alpha(data,delete=TRUE,check.keys=TRUE)
#+END_SRC

#+RESULTS:
#+begin_example

Reliability analysis   
Call: alpha(x = data, check.keys = TRUE, delete = TRUE)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd
      0.28      0.82    0.85       0.2 4.6 0.064  7.7 2.6

 lower alpha upper     95% confidence boundaries
0.15 0.28 0.41 

 Reliability if an item is dropped:
          raw_alpha std.alpha G6(smc) average_r S/N alpha se
Student.-      0.80      0.83    0.85      0.22 4.9    0.021
Q1             0.18      0.80    0.83      0.19 3.9    0.064
Q2             0.27      0.81    0.83      0.20 4.1    0.064
Q3             0.26      0.80    0.83      0.19 3.9    0.064
Q4             0.26      0.80    0.83      0.19 4.1    0.064
Q5             0.26      0.80    0.83      0.19 4.0    0.064
Q6             0.27      0.80    0.83      0.19 4.0    0.064
Q7             0.27      0.81    0.84      0.20 4.2    0.064
Q8             0.27      0.81    0.84      0.20 4.2    0.060
Q9             0.27      0.82    0.85      0.21 4.6    0.064
Q10            0.27      0.82    0.84      0.21 4.4    0.064
Q11            0.28      0.82    0.85      0.22 4.7    0.064
Q12            0.28      0.83    0.85      0.22 4.7    0.063
Q13            0.26      0.81    0.84      0.20 4.4    0.065
Q14            0.29      0.83    0.86      0.23 5.0    0.064
Q15            0.28      0.82    0.85      0.22 4.7    0.063
Q16            0.27      0.80    0.83      0.19 4.0    0.063
Q17            0.20      0.80    0.83      0.19 3.9    0.065

 Item statistics 
            n  raw.r std.r r.cor r.drop mean   sd
Student.- 130  0.865  0.16 0.078  0.089 64.5 37.7
Q1        130  0.570  0.74 0.749  0.464 10.6  6.4
Q2        130  0.320  0.60 0.579  0.299  4.0  1.1
Q3        130  0.392  0.73 0.734  0.358  4.3  1.8
Q4        130  0.355  0.63 0.617  0.315  6.6  2.1
Q5        130  0.328  0.70 0.703  0.293  3.9  1.8
Q6        130  0.394  0.66 0.647  0.373  2.1  1.2
Q7        130  0.256  0.57 0.545  0.223  2.1  1.6
Q8        130  0.213  0.54 0.509  0.125 13.2  4.1
Q9        130  0.240  0.34 0.266  0.188  2.8  2.5
Q10       130  0.234  0.43 0.368  0.182  2.6  2.5
Q11       130  0.104  0.29 0.209  0.055  3.6  2.3
Q12       130  0.072  0.27 0.180  0.024  1.3  2.2
Q13       130  0.347  0.48 0.427  0.298  2.3  2.5
Q14       130 -0.014  0.12 0.022 -0.059  1.0  2.0
Q15       130  0.088  0.30 0.219  0.042  1.1  2.1
Q16       130  0.242  0.66 0.654  0.215  3.5  1.3
Q17       130  0.573  0.73 0.729  0.498  8.3  4.7
#+end_example

I confess the alpha is not completley clear to me. Here I am submitting the results as it is.

** Testing for normality
Apply Ks test.

Please note: if you want to test for normality in an experiment with more than one group, you’ll have to run separate analyses for the each group. It’s important the distribution of each group is normal, rather than the distribution of the scores of the two groups taken together.

#+BEGIN_SRC R :session :exports both :results output
data <- read.csv("./data/p3a.csv",na="",header=TRUE)
data$totalscore <- rowSums(data[,4:20])
ks.test((data$totalscore),"pnorm")
#+END_SRC

#+RESULTS:
#+begin_example

	One-sample Kolmogorov-Smirnov test

data:  (data$totalscore)
D = 1, p-value < 2.2e-16
alternative hypothesis: two-sided

Warning message:
In ks.test((data$totalscore), "pnorm") :
  ties should not be present for the Kolmogorov-Smirnov test
#+end_example

It seems that there are repeated values in the =TOTALscore= variable. In fact, shouldn't the KS test be applied to continous distributions only ? In the next section I run it on unique elements of TOTALscore and it correclty reports no warning. Maybe is SPSS doing this automatically?

#+BEGIN_SRC R :session :exports both :results output
data <- read.csv("./data/p3a.csv",na="",header=TRUE)
data$totalscore <- rowSums(data[,4:20])
ks.test(unique(data$totalscore),"pnorm")
#+END_SRC

#+RESULTS:
: 
: 	One-sample Kolmogorov-Smirnov test
: 
: data:  unique(data$totalscore)
: D = 1, p-value = 2.22e-16
: alternative hypothesis: two-sided

* Practical 5
** Weight and height

#+BEGIN_SRC R :session :exports both :results valie
w <- c(40,50,40,70,80,90)
h <- c(1.40,1.50,1.60,1.70,1.80,1.90)
df <-data.frame(w,h)
#+END_SRC

#+RESULTS:
| 40 | 1.4 |
| 50 | 1.5 |
| 40 | 1.6 |
| 70 | 1.7 |
| 80 | 1.8 |
| 90 | 1.9 |

*** Questions
**** List the variables in the study – if relevant, say which variables are dependent and which are independent
Variables are /weight/ and /height/.
**** What kind of measures (nominal, ordinal, interval) are used for the variables ?
Interval scale  
**** Formulate the relevant statistical hypothesis
H_0: the two variables are not correlated
H_1: the two variable are correlated
**** Is the relation linear? (plot the data in a simple graph)

#+BEGIN_SRC R :session :file wh.png :results value graphics :exports both
plot(df)
#+END_SRC

#+RESULTS:
[[file:wh.png]]
Yes, there seem to be a strong linear relation
**** Which α-level would you use and why?
.5

It is the standard alfa and I see no reason to do otherwise. 
**** Would you test one-tailed or two-tailed (and why)?
Given that we are going to run a correlation test I don't see how we can test one tail.
**** Which statistic could be used? (consult the tables onyour handout)
Correlation
**** Apply this statistic using SPSS/R. Can you reject H_0?
#+BEGIN_SRC R :session :exports both :results output
cor.test(df$w,df$h, method="pearson")
#+END_SRC

#+RESULTS:
#+begin_example

	Pearson's product-moment correlation

data:  df$w and df$h
t = 4.8865, df = 4, p-value = 0.008122
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.4576706 0.9919809
sample estimates:
      cor 
0.9254821
#+end_example

**** What can you say about the meaningfulness of this outcome?
Not much at all. We have very few data.
**** Report
#+BEGIN_QUOTE
A correlation analysis showed that weight and height were significantly related (r=0.92, p < 0.5)
#+END_QUOTE
** Writing scores
#+BEGIN_SRC R :session :exports both :results value
partecipant <- c(1:30)
type <- c(0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2,0,1,2)
type <- as.factor(type)
levels(type) <- c("no_instr","lectures","gw")
scores <- c(34,65,68,58,54,87,56,43,94,47,57,69,35,65,81,31,49,75,55,74,94,65,79,78,61,54,63,27,65,78)
df <-data.frame(partecipant,type,scores)
head(df)
#+END_SRC

#+RESULTS:
| 1 | no_instr | 34 |
| 2 | lectures | 65 |
| 3 | gw       | 68 |
| 4 | no_instr | 58 |
| 5 | lectures | 54 |
| 6 | gw       | 87 |

*** Questions
**** List the variables in the study –if relevant, say which variables are dependent and which are independent.
#+BEGIN_SRC R :session :exports both :results output
str(df)
#+END_SRC 

#+RESULTS:
: 'data.frame':	30 obs. of  3 variables:
:  $ partecipant: int  1 2 3 4 5 6 7 8 9 10 ...
:  $ type       : Factor w/ 3 levels "no_instr","lectures",..: 1 2 3 1 2 3 1 2 3 1 ...
:  $ scores     : num  34 65 68 58 54 87 56 43 94 47 ...



There is one independet (type of instruction) and one dependent (score).
**** What kind of measures (nominal, ordinal, interval) are used for the variables ?
Interval scale, as the description of the dataset says.
**** In case of independent variables, how many levels does each independent variable have?
Three: /no instructions/, /lectures/, /guided instr/
**** Formulate a statistical hypothesis.
- H_0: the type of istruction does not affect the score
- H_1: the type of instruction does affect the score
**** Which statistic could be used?
One way ANOVA (eventually with post-hoc analysis).
**** Using SPSS/R, provide the following descriptive statistics for each group: means, range, standard deviations.

#+BEGIN_SRC R :session :exports both :results output
bymean <- aggregate(df$score, by=list(df$type),FUN=mean)
byrange <- aggregate(df$score, by=list(df$type),FUN=range)
bysd <- aggregate(df$score, by=list(df$type),FUN=sd)
bymean
byrange
bysd
#+END_SRC 

#+RESULTS:
#+begin_example
   Group.1    x
1 no_instr 46.9
2 lectures 60.5
3       gw 78.7
   Group.1 x.1 x.2
1 no_instr  27  65
2 lectures  43  79
3       gw  63  94
   Group.1        x
1 no_instr 13.96384
2 lectures 11.15796
3       gw 10.60451
#+end_example

**** Using SPSS/R, test the statistical significance of this experiment: can you reject H 0 ?
Before looking at the F score, it might be a good idea to plot the results (even, if it is not required in this excercise I find it always useful to plot things).

#+BEGIN_SRC R :session :file writingscores.png :results value graphics :exports both
boxplot(df$score[df$type == "no_instr"],df$score[df$type == "lectures"],df$score[df$type == "gw"])
#+END_SRC

#+RESULTS:
[[file:crosstabs1.png]]

And indeed it seems obvious that the guided group performs a lot better than the other two, so there the teaching method affects the results (confirming what one's intuitions) and so H_0 can be rejected. Let's run the ANOVA anyway and take a look at the F.

#+BEGIN_SRC R :session :exports both :results output
results = aov(scores ~ type, data=df)
summary(results)
#+END_SRC 

#+RESULTS:
:             Df Sum Sq Mean Sq F value   Pr(>F)    
: type         2   5091    2546   17.68 1.24e-05 ***
: Residuals   27   3887     144                     
: ---
: Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

**** What can you say about the meaningfulness of this outcome?
The results seems to be meaningful. However, the sample is still small.
**** Report
  #+BEGIN_QUOTE
  On average, the =gw= group (M=78.7) performed better than the other two groups. This difference was significant p < 0.05.
  #+END_QUOTE

** Crosstabs
#+BEGIN_SRC R :session :exports both :results output
data <- read.csv("./data/p5a3.csv")
colnames(data) <- c("class", "reply")
data$class <- as.factor(data$class)
data$reply <- as.factor(data$reply)
levels(data$class) <- c("high","low")
levels(data$reply) <- c("havent","dont")
str(data)
head(data)
#+END_SRC 

#+RESULTS:
#+begin_example
'data.frame':	224 obs. of  2 variables:
 $ class: Factor w/ 2 levels "high","low": 1 1 1 1 1 1 1 1 1 1 ...
 $ reply: Factor w/ 2 levels "havent","dont": 1 1 1 1 1 1 1 1 1 1 ...
  class  reply
1  high havent
2  high havent
3  high havent
4  high havent
5  high havent
6  high havent
#+end_example

*** Questions
**** List the variables included in this study.
There are two nominal variables: social class and reply
**** For each variable, say what its function is (dependent, independent, etc.)  and its type (nominal, ordinal, interval).
=Social class= is the independent whereas =reply= is the dependent
**** How would you formulate H_0 and H_1 ?
- H_0: the social status does not influence language use
- H_1: the social status does influence language use
**** Which statistic could be used?
chi-square, since there are two nominal variables.
**** Choose your α-level
The usual .5
**** Using the data file provided (p5a3.csv) , run the SPSS/R analysis.
#+BEGIN_SRC R :session :exports both :results output
chisq.test(data$class,data$reply)
#+END_SRC 

#+RESULTS:
: 
: 	Pearson's Chi-squared test with Yates' continuity correction
: 
: data:  data$class and data$reply
: X-squared = 3.3829, df = 1, p-value = 0.06588

**** Can you reject the null hypothesis?
Looking at the p-value it seems we cannot. But let's plot first.

#+BEGIN_SRC R :session :file crosstabs1.png :results value graphics :exports both
spineplot(data$class,data$reply)
#+END_SRC

#+RESULTS:
[[file:crosstabs1.png]]

And yes, the plot confirms that in fact we can't reject the null. However, the high class prefers one form over another. So, maybe with more data the result could change ?
 
**** Report
  #+BEGIN_QUOTE
  On average, the high class group showed a preference for the =haven't= form. This difference was not significant X-square=3.38, p > 0.05
  #+END_QUOTE

** Exam practice
Tempate answer

#+BEGIN_EXAMPLE
- List the variables in the study – if relevant, say which variables are dependent and which are independent 
- For each of the variables determine its type (nominal, ordinal, scale) 
- In case of independent variables, how many levels does each independent variable h ave?  
- Identify the family of statistics: means , frequency or correlation ; then choose the most appropriate statistic al test.  
- Formulate the relevant statistical hypothesis 
- Which α-level would you use and why?  
- Would you test one-tailed or two-tailed (and why)?  
- Value of statistic: 
- Significance: 
- Decision H 0:
- Report:
  #+BEGIN_QUOTE
  “ A correlation analysis showed that Reading Skills and Listening Skills were .... [significantly or not significantly] related (r = ..., p ... [ fill in < 0.05 or > 0.05 or whichever α you’ve selected] ) ”
  #+END_QUOTE

  #+BEGIN_QUOTE
  “ On average, the ... [fill in boys or girls] showed a higher level of intelligence (M=... , SE= ... )  than the ... (M=... , SE= ... ). This difference was ... [fill in “significant” or “not significant”] t(...[fill in df])= ... [fill in the value of t], p ... [fill in < 0.05 o r > 0.05 or whichever α you’ve selected].  ”
  #+END_QUOTE

#+END_EXAMPLE

*** A) Pronunciation
A researcher wants to investigate if motivation affects the pronunciation of English by Dutch learners. To investigate the possible effect of motivation on pronunciation, she makes tape recordings of 24 Dutch learners of English pronouncing English sentences. She then measures the difference in vowel length before voiced and voiceless obstruents (e.g. tap vs. tab). A questionnaire has determined that 12 of these students are highly motivated and 12 students are not very motivated to pronounce English correctly. Tip: the dependent is the DIFFERENCE in vowel length between the two phonological contexts.
**** Answer
- List the variables in the study – if relevant, say which variables are dependent and which are independent
  + one dependent and one independent
- For each of the variables determine its type (nominal, ordinal, scale) 
  + the dependent is interval scale
  + independent is nominal
- In case of independent variables, how many levels does each independent variable h ave?  
  + two levels: highly motivated, not very motivated
- Identify the family of statistics: means , frequency or correlation ; then choose the most appropriate statistical test.
  + means analysis
  + t-test
- Formulate the relevant statistical hypothesis
  + H_0: motivation does not affect pronunciation skills
  + H_1: motivation does affect pronunciation skills
- Which α-level would you use and why?  
  + .5
  + it's the standard
- Would you test one-tailed or two-tailed (and why)?
  + two tail
  + can't see an obvious resons for not doing so
- Value of statistic: t
- Significance: p
- Decision H 0: I don't understand this question

*** B) Polish learning French
A researcher wants to find out whether the age at which one starts to learn a foreign language is related to language proficiency. To investigate this, she finds 20 Polish learners of French who had all been learning French for 10 years. The starting age of these learners ranges from 1 to 20, in such a way that each starting age is included precisely once. All learners take a 50-item French proficiency test; the proficiency score is based on the number of correct item
**** Answer
- List the variables in the study – if relevant, say which variables are dependent and which are independent
  + one dependent (/proficiency score/) and one independent (/starting age/)
- For each of the variables determine its type (nominal, ordinal, scale) 
  + both are scale
- In case of independent variables, how many levels does each independent variable h ave?  
  ~
- Identify the family of statistics: means , frequency or correlation ; then choose the most appropriate statistical test.
  + correlation
  + Pearson's R
- Formulate the relevant statistical hypothesis
  + H_0: starting does not affect language learning
  + H_1: starting age does affect language learning
- Which α-level would you use and why?  
  + .5
  + it's the standard
- Would you test one-tailed or two-tailed (and why)?
  + two tail
  + can't see an obvious resons for not doing so
- Value of statistic: r
- Significance: p
- Decision H 0: ???

*** C) Poor Japanese
To investigate the effect of input on sec ond language learning, 60 randomly selected Japanese learners of Frisian are divided into two groups: one experimental group of 30 is isolated in a dark room and exposed to Omrop Fryslân 24 hours a day (thereby achieving maximum exposure to Frisian); one control group of 30 is not exposed to Frisian. After two months, both groups are submitted to a 100-item Frisian proficiency test; the proficiency score is based on the number of correct items.
**** Answer
- List the variables in the study – if relevant, say which variables are dependent and which are independent
  + one dependent (/proficiency score/) and one independent (/exposure/)
- For each of the variables determine its type (nominal, ordinal, scale) 
  + the dependent is a internaval scale variable
  + the independent is a nominal variable
- In case of independent variables, how many levels does each independent variable h ave?  
  + two levels: /exposure/ and /non exposure/
- Identify the family of statistics: means , frequency or correlation ; then choose the most appropriate statistical test.
  + means analysis
  + t-test
- Formulate the relevant statistical hypothesis
  + H_0: exposure does not affect language learning
  + H_1: exposure age does affect language learning
- Which α-level would you use and why?  
  + .5
  + it's the standard
- Would you test one-tailed or two-tailed (and why)?
  + two tail
  + can't see an obvious resons for not doing so
- Value of statistic: t
- Significance: p
- Decision H 0: ???

*** D) Again
The experiment in c) is done once more, but this time each of the groups is equally subdivided into three age groups: 11 -30, 31-50 and 51-70.  Does age influence the exposure[/sic/: proficiency intended?]?
**** Answer
Could be.

In this case we would have to use a one-way ANOVA and all the rest is the same.

*** E) Toddlers
A researcher was interested in the effects of social reinforcement on toddlers’ motor skills. In an experiment, 56 three-year-old children had to take marbles from a vase and put them into a box through a tiny hole. The number of marbles was counted that had been put into the box after four minutes. The children were randomly attributed to two groups. In a 10 minute learning period preceding the experiment, the children in the first group were encouraged by smiles and words of praise. The children in the second group were not encouraged.
**** Answer
- List the variables in the study – if relevant, say which variables are dependent and which are independent
  + one dependent ( /number of marbles put in the box/ ) and one independent (/encouragement/)
- For each of the variables determine its type (nominal, ordinal, scale) 
  + the dependent is a internaval scale variable
  + the independent is a nominal variable
- In case of independent variables, how many levels does each independent variable h ave?  
  + two levels: /praise/ vs. /non praise/
- Identify the family of statistics: means , frequency or correlation ; then choose the most appropriate statistical test.
  + means analysis
  + t-test
- Formulate the relevant statistical hypothesis
  + H_0: encouragmenet does not affect toddler's skills
  + H_1: encouragmenet does affect toddler's skills
- Which α-level would you use and why?  
  + .5
  + it's the standard
- Would you test one-tailed or two-tailed (and why)?
  + two tail
  + can't see an obvious resons for not doing so
- Value of statistic: t
- Significance: p
- Decision H 0: ???

*** F) Toddlers again
In what way would the experiment in e ) change if, in addition, the researcher wanted to find out if social reinforcement equally affects the boys and girls in the experiment?  Reconsider the number and type of variables accordingly, and decide on the type of analysis that would be required for this new situation.

**** Answer
In this case we would have two nominal independent variables and one dependent. So, 4 groups in total: this case can be modeled using a two-way ANOVA. 
*** G) Sport and stress
To investigate the relation between active sports performance and stress a questionnaire is set up. The questionnaire determines if the participants are active sportswomen and sportsmen (Yes or No) and the degree of stress they experience in their daily lives (on a 3-point scale).
**** Answer
- List the variables in the study – if relevant, say which variables are dependent and which are independent
  + one dependent (/stress levels/) and one independent (/activity/)
- For each of the variables determine its type (nominal, ordinal, scale) 
  + the dependent is a nominal variable
  + the independent is a nominal variable
- In case of independent variables, how many levels does each independent variable h ave?  
  + two levels: /praise/ vs. /non praise/
- Identify the family of statistics: means , frequency or correlation ; then choose the most appropriate statistical test.
  + frequency distribution
  + chi-square
- Formulate the relevant statistical hypothesis
  + H_0: sport does not affect perceived stress
  + H_1: sport does affect perceived stress
- Which α-level would you use and why?  
  + .5
  + it's the standard
- Would you test one-tailed or two-tailed (and why)?
  + two tail
  + can't see an obvious resons for not doing so
- Value of statistic: 
- Significance: ???
- Decision H 0: 
  
* Practical 6
** Instructions

| variables               |   |
| variables type          |   |
| variables function      |   |
| H_0                     |   |
| H_1                     |   |
| one- vs. two- tail test |   |
| statistics              |   |
| comments                |   |


** L2 syntax development

#+BEGIN_SRC R :session :exports both :results output
pupil <- c(1:13)
minutes <- c(32,76,89,41,17,47,62,81,93,56,68,71,26)
grade <- c(4,5,9,6,4,7,7,8,8,6,8,8,6)
df <- data.frame(pupil,minutes,grade)
head(df)
str(df)
#+END_SRC

#+RESULTS:
#+begin_example
  pupil minutes grade
1     1      32     4
2     2      76     5
3     3      89     9
4     4      41     6
5     5      17     4
6     6      47     7
'data.frame':	13 obs. of  3 variables:
 $ pupil  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ minutes: num  32 76 89 41 17 47 62 81 93 56 ...
 $ grade  : num  4 5 9 6 4 7 7 8 8 6 ...
#+end_example

#+BEGIN_SRC R :session :exports both :results output
summary(df$minutes)
summary(df$grade)
#+END_SRC

#+RESULTS:
:    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
:   17.00   41.00   62.00   58.38   76.00   93.00
:    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
:   4.000   6.000   7.000   6.615   8.000   9.000

Before doing any statistics a simple plot reveal that there is an evident trend in the (small) dataset we have.

#+BEGIN_SRC R :session :file minutesgrades.png :results value graphics :exports both
plot(df$minutes,df$grade)
#+END_SRC

I am going to interpret the grad variable as an interval variable.

#+BEGIN_SRC R :session :exports both :results output
cor.test(df$minutes,df$grade,method="pearson")
#+END_SRC

#+RESULTS:
#+begin_example

	Pearson's product-moment correlation

data:  df$minutes and df$grade
t = 3.9701, df = 11, p-value = 0.002196
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.3750408 0.9266102
sample estimates:
      cor 
0.7674407
#+end_example

Now, as expected we have a strong correlation, which is not significative. I assume this is due to the fact the sample size is really small.

*** Report

**** Variables

| variables               | minutes                                                                    | grade     |
| variables type          | interval scale                                                             | interval  |
| variables function      | independent                                                                | dependent |
|-------------------------+----------------------------------------------------------------------------+-----------|

**** Experiment features

| H_0                     | time affects skills                                                        |
| H_1                     | time does not affect skills (improves)                                     |
| H_2                     | time does not affect skills (decreses)                                     |
| one- vs. two- tail test | two tail                                                                   |
| statistics              | correlation                                                                |
| comments                | not enough data, evaluation explicitlye based on N correct items desirable |


** Spanish pronunciation proficiency

#+BEGIN_SRC R :session :exports both :results output
studentID <- c(1:50)
abroad <- c("no","yes","yes","no","yes","no","no","yes","yes","no","yes","yes","yes","yes","yes","yes","yes","no","yes","no","no","no","no","yes","yes","yes","yes","no","no","no","yes","yes","no","no","no","yes","no","yes","yes","no","yes","yes","no","no","yes","yes","no","yes","no","yes")
pass <- c("no","no","yes","no","no","no","yes","no","yes","no","yes","yes","no","yes","yes","no","yes","no","yes","no","no","yes","yes","yes","yes","no","yes","no","no","yes","yes","no","yes","yes","no","yes","no","no","yes","yes","yes","no","no","yes","yes","no","no","yes","no","yes")
df = data.frame(studentID,abroad,pass)
str(df)
#+END_SRC

#+RESULTS:
: 'data.frame':	50 obs. of  3 variables:
:  $ studentID: int  1 2 3 4 5 6 7 8 9 10 ...
:  $ abroad   : Factor w/ 2 levels "no","yes": 1 2 2 1 2 1 1 2 2 1 ...
:  $ pass     : Factor w/ 2 levels "no","yes": 1 1 2 1 1 1 2 1 2 1 ...

As always, we start by plotting the data.

#+BEGIN_SRC R :session :file spanishabroad.png :results value graphics :exports both
spineplot(df$abroad,df$pass)
#+END_SRC

#+RESULTS:
[[file:spanishabroad.png]]

#+BEGIN_SRC R :session :exports both :results output
chisq.test(df$abroad,df$pass)
#+END_SRC

#+RESULTS:
: 
: 	Pearson's Chi-squared test with Yates' continuity correction
: 
: data:  df$abroad and df$pass
: X-squared = 2.8109, df = 1, p-value = 0.09363

*** ☛ TODO Report

**** Variables

| variables               | minutes                                                                    | grade     |
| variables type          | interval scale                                                             | interval  |
| variables function      | independent                                                                | dependent |
|-------------------------+----------------------------------------------------------------------------+-----------|

**** Experiment features

| H_0                     | time affects skills                                                        |
| H_1                     | time does not affect skills (improves)                                     |
| H_2                     | time does not affect skills (decreses)                                     |
| one- vs. two- tail test | two tail                                                                   |
| statistics              | correlation                                                                |
| comments                | not enough data, evaluation explicitlye based on N correct items desirable |


** Vocabulary learning experiment

#+BEGIN_SRC R :session :exports both :results output
data <- read.csv("./data/p6a3.csv",sep=";")
str(data)
#+END_SRC

#+RESULTS:
#+begin_example
'data.frame':	59 obs. of  6 variables:
 $ no   : int  1 2 3 4 5 6 7 8 9 10 ...
 $ code : Factor w/ 59 levels "AJEU","ALYA",..: 45 26 18 5 33 15 38 10 16 52 ...
 $ ela  : int  1 1 1 1 1 1 1 1 1 1 ...
 $ Guess: int  78 50 56 22 39 22 17 44 39 28 ...
 $ STret: int  94 56 100 67 100 100 89 78 89 100 ...
 $ LTret: int  78 39 83 67 89 NA 61 33 72 56 ...
   no code ela Guess STret LTret
54 54 BDUS   2    67   100   100
55 55 LKRO   2    33   100    56
56 56  KLM   1    45   100    85
57 57  RTL   2    55    93    43
58 58  TWL   2    53    87    35
59 59  PLE   1    65   100    81
#+end_example


Let's see how the data looks like.

#+BEGIN_SRC R :session :exports both :results output
head(data)
#+END_SRC

#+RESULTS:
:   no code ela Guess STret LTret
: 1  1 RIWE   1    78    94    78
: 2  2 LBRA   1    50    56    39
: 3  3 HWOL   1    56   100    83
: 4  4 AWES   1    22    67    67
: 5  5 MMUN   1    39   100    89
: 6  6 FVEL   1    22   100    NA

The first lines seem to confirm what one might expect: the =G= looks the lowest ones, =STret= is the vector containing the highest scores and =LTret= is in the middle.

Let's if a plot confirms this.

#+BEGIN_SRC R :session :file coreperi.png :results value graphics :exports both
boxplot(data$Guess,data$STret,data$LTret)
#+END_SRC

#+RESULTS:
[[file:coreperi.png]]


** Vocabulary scores and instruction
#+BEGIN_SRC R :session :exports both :results output
data <- read.csv("./data/p6a4.csv",sep=";")
str(data)
#+END_SRC

#+RESULTS:
: 'data.frame':	42 obs. of  3 variables:
:  $ Subject : int  1 2 3 4 5 6 7 8 9 10 ...
:  $ pretest : int  9 10 8 3 8 9 4 12 10 13 ...
:  $ posttest: int  3 15 12 9 10 8 11 10 11 14 ...

#+BEGIN_SRC R :session :exports both :results output
head(data)
#+END_SRC

#+RESULTS:
:   Subject pretest posttest
: 1       1       9        3
: 2       2      10       15
: 3       3       8       12
: 4       4       3        9
: 5       5       8       10
: 6       6       9        8

#+BEGIN_SRC R :session :file kidsenglish.png :results value graphics :exports both
boxplot(data$pretest,data$posttest)
#+END_SRC

#+RESULTS:
[[file:kidsenglish.png]]

* Notes								   :noexport:
** Always check for normality before running t-test
** Equality of variance 
A rule of thumb with equality of variance is that the largest SD of your groups should not be more than twice the smallest SD of your groups.
** Reporting guidelines
http://my.ilstu.edu/~jhkahn/apastats.html
