#+TITLE: Basic statistics
#+AUTHOR: Angelo Basile
#+EMAIL: me@angelobasile.it

* Practical 1
Firs we begin be importing our dataset. We have to use an additional library to read an *.xlsx file. After that we assign names to the columns. And we take a look at the first rows of the dataset to see what it looks like.

#+BEGIN_SRC R :session :exports both
data <- read.csv("./data/p1.csv")
names(data) <- c("id","age","sex","profs")
head(data)
#+END_SRC

#+RESULTS:
| 1 | 16 | 1 | 91 |
| 2 | 20 | 2 | 58 |
| 3 | 24 | 1 | 52 |
| 4 | 22 | 2 | 45 |
| 5 | 18 | 1 | 78 |
| 6 | 14 | 2 | 88 |

** Question 1
/Does the proficiency score increase or decrease with age?/

To answer this question we might simply plot both age and proficiency score and see if there is any clear indication.

#+name: fig1
#+BEGIN_SRC R :session :file 1.png :results value graphics :exports both
plot(data$age,data$profs)
#+END_SRC

#+RESULTS: fig1
[[file:2.png]]

Well, yes, from the scatter plot we built we can see that as the age increases the proficiency decreases.

** Question 2
/Is there a difference in proficiency score between female and male participants?/

#+BEGIN_SRC R :session :exports both 
aggregate(data$profs, by=list(Category=data$sex), FUN=mean)
#+END_SRC

#+RESULTS:
| 1 | 75.5333333333333 |
| 2 | 63.6666666666667 |

** Question 3
/Is there an overall difference in age between male and female participants?/

#+BEGIN_SRC R :session :file 2.png :results value graphics
m <- data$age[data$sex == 1]
f <- data$age[data$sex == 2]
boxplot(m,f)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :session :exports both
aggregate(data$age, by=list(Category=data$sex), FUN=mean)
#+END_SRC

#+RESULTS:
| 1 | 20.1333333333333 |
| 2 | 20.5333333333333 |

* Practical 2

#+BEGIN_SRC R :session :exports both
data <- read.csv("./data/p1.csv")
names(data) <- c("id","age","sex","profs")
str(data)
#+END_SRC

#+RESULTS:

** Which age occurs most often?
#+BEGIN_SRC R :session :exports both 
which.max(tabulate(data$age))
#+END_SRC

#+RESULTS:
: 16

** Find out what’s the mean, the median, the mode, the range and the standard deviation of the Proficiency Score in your data.

#+BEGIN_SRC R :session :exports both 
mean(data$profs)
#+END_SRC

#+RESULTS:
: 69.6

#+BEGIN_SRC R :session :exports both
median(data$profs)
#+END_SRC

#+RESULTS:
: 69

#+BEGIN_SRC R :session :exports both
which.max(tabulate(data$profs))
#+END_SRC

#+RESULTS:
: 59

#+BEGIN_SRC R :session :exports both
range(data$profs)
#+END_SRC

#+RESULTS:
| 33 |
| 97 |

#+BEGIN_SRC R :session :exports both
sd(data$profs)
#+END_SRC

#+RESULTS:
: 16.4434244273069


** Find out what is the minimum age, the maximum age, the mean age and the standard deviation.
#+BEGIN_SRC R :session :exports both
min(data$age)
#+END_SRC

#+RESULTS:
: 14

#+BEGIN_SRC R :session :exports both
max(data$age)
#+END_SRC

#+RESULTS:
: 27

#+BEGIN_SRC R :session :exports both
mean(data$age)
#+END_SRC

#+RESULTS:
: 20.3333333333333

#+BEGIN_SRC R :session :exports both
sd(data$age)
#+END_SRC

#+RESULTS:
: 3.56547944576335

#+BEGIN_SRC R :session :exports both
sd(data$age)
#+END_SRC

#+RESULTS:
: 3.56547944576335

** What is the most frequently occurring proficiency score
#+BEGIN_SRC R :session :exports both
which.max(tabulate(data$profs))
#+END_SRC

#+RESULTS:
: 59

** What is the z-score of Participant 13?
#+BEGIN_SRC R :session :exports both 
scale(data$profs,center=TRUE, scale=TRUE)[13]
#+END_SRC

#+RESULTS:


** Which group has higher proficiency scores, the male or the female participants?
#+BEGIN_SRC R :session :exports both 
aggregate(data$profs, by=list(Category=data$sex), FUN=mean)
#+END_SRC

#+RESULTS:
| 1 | 75.5333333333333 |
| 2 | 63.6666666666667 |

** Which group scored more homogeneously?

#+BEGIN_SRC R :session :exports both 
aggregate(data$profs, by=list(Category=data$sex), FUN=sd)
#+END_SRC

#+RESULTS:
| 1 | 14.2421239721502 |
| 2 | 16.7871833197092 |
** Boxplots
#+BEGIN_SRC R :session :file 3.png :results value graphics :exports both
m <- data$profs[data$sex == 1]
f <- data$profs[data$sex == 2]
boxplot(m,f,names=c("M","F"))
#+END_SRC

#+RESULTS:
[[file:3.png]]

** Part B
Provide the mean, the mode, the median, the range and the standard deviation.
#+BEGIN_SRC R :session :exports both
a <- c(3, 4, 5, 6, 7, 8, 9)
b <- c(6, 6, 6, 6, 6, 6, 6)
c <- c(4, 4, 4, 6, 7, 7, 10)
d <- c(1, 1, 1, 4, 9, 12, 14)
#+END_SRC

#+BEGIN_SRC R :session :exports both
MySummary <- function(dataset) {
  m = mean(dataset)
  mode = which.max(tabulate(dataset))
  med = median(dataset)
  stdde = sd(dataset)
  results <- c(m,mode,med,stdde)
  return(results)
}

#+END_SRC

#+BEGIN_SRC R :session :exports both
MySummary(a)
#+END_SRC

#+BEGIN_SRC R :session :exports both
MySummary(b)
#+END_SRC

#+BEGIN_SRC R :session :exports both
MySummary(c)
#+END_SRC

#+BEGIN_SRC R :session :exports both
MySummary(d)
#+END_SRC
* Practical 3
** Part A

As always, we begin by importing the data and taking a quick look at the first rows to see what it looks like.
#+BEGIN_SRC R :session :exports both
data <- read.csv("./data/p3a.csv",na="",header=TRUE)
head(data)
#+END_SRC

#+RESULTS:
| 1 | A | 1A | 10 | 5 | 5 | 7 | 4 | 2 | 4 | 14 | 5 | 5 | 5 | 0 | 5 | 0 | 0 | 4 | 12 |
| 2 | A | 1A | 12 | 5 | 4 | 8 | 4 | 4 | 5 | 18 | 5 | 0 | 5 | 0 | 0 | 5 | 0 | 4 | 17 |
| 3 | A | 1A | 10 | 4 | 5 | 6 | 2 | 3 | 0 |  8 | 0 | 5 | 5 | 0 | 5 | 0 | 0 | 4 | 11 |
| 4 | A | 1A | 18 | 5 | 6 | 8 | 5 | 3 | 4 | 15 | 5 | 5 | 5 | 0 | 5 | 0 | 5 | 4 | 12 |
| 5 | A | 1B | 20 | 5 | 6 | 7 | 5 | 4 | 4 | 19 | 5 | 5 | 5 | 0 | 5 | 0 | 0 | 5 | 13 |
| 6 | A | 1A | 16 | 5 | 6 | 8 | 6 | 3 | 1 | 19 | 0 | 0 | 5 | 0 | 5 | 0 | 0 | 4 | 11 |

Now we define the type of variables for =teacher= and =group=. More precisely, we want to define them as /factors/.
#+BEGIN_SRC R :session :exports both :results output
data$group <- as.factor(data$group)
data$teacher <- as.factor(data$teacher)
str(data)
#+END_SRC

#+RESULTS:
#+begin_example
Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	130 obs. of  20 variables:
 $ Student#: chr  "1" "2" "3" "4" ...
 $ teacher : Factor w/ 2 levels "A","B": 1 1 1 1 1 1 1 1 1 1 ...
 $ group   : Factor w/ 5 levels "1A","1B","1C",..: 1 1 1 1 2 1 1 1 1 1 ...
 $ Q1      : num  10 12 10 18 20 16 10 7 20 11 ...
 $ Q2      : num  5 5 4 5 5 5 3 4 4 5 ...
 $ Q3      : num  5 4 5 6 6 6 4 6 6 5 ...
 $ Q4      : num  7 8 6 8 7 8 8 6 6 7 ...
 $ Q5      : num  4 4 2 5 5 6 5 3 6 6 ...
 $ Q6      : num  2 4 3 3 4 3 3 2 3 3 ...
 $ Q7      : num  4 5 0 4 4 1 3 0 4 2 ...
 $ Q8      : num  14 18 8 15 19 19 16 14 17 17 ...
 $ Q9      : num  5 5 0 5 5 0 0 0 5 5 ...
 $ Q10     : num  5 0 5 5 5 0 5 0 5 5 ...
 $ Q11     : num  5 5 5 5 5 5 5 5 5 5 ...
 $ Q12     : num  0 0 0 0 0 0 5 0 0 0 ...
 $ Q13     : num  5 0 5 5 5 5 5 0 5 0 ...
 $ Q14     : num  0 5 0 0 0 0 0 0 0 0 ...
 $ Q15     : num  0 0 0 5 0 0 0 0 5 0 ...
 $ Q16     : num  4 4 4 4 5 4 2 5 4 4 ...
 $ Q17     : num  12 17 11 12 13 11 11 2 8 12 ...
#+end_example

*** Descriptives and graphs for groups
Adding a =TOTAL_score= variable.
#+BEGIN_SRC R :session :exports both :results output
data$TOTAL_score <- rowSums(data[,4:20])
str(data$TOTAL_score)
#+END_SRC

#+RESULTS:
:  num [1:130] 87 96 68 105 108 89 85 54 103 87 ...

*** Which performed best? And which group performed most homogeneously?
#+BEGIN_SRC R :session :exports both
best <- aggregate(data$TOTAL_score, by=list(data$group), FUN=mean)
best$Group.1[which.max(best$x)]
#+END_SRC  

#+RESULTS:
: 1B

#+BEGIN_SRC R :session :exports both
more_homo <- aggregate(data$TOTAL_score, by=list(data$group), FUN=sd)
more_homo$Group.1[which.min(more_homo$x)]
#+END_SRC

#+RESULTS:
: 1C

*** Which teacher performed best?
#+BEGIN_SRC R :session :exports both
byteacher <- aggregate(data$TOTAL_score, by=list(data$teacher), FUN=mean)
#+END_SRC

#+RESULTS:
| A |             78.4 |
| B | 69.2928571428571 |

Teacher A

*** Boxplot
#+BEGIN_SRC R :session :file teacher.png :results value graphics :exports both
teacherA <- data$TOTAL_score[data$teacher == "A"]
teacherB <- data$TOTAL_score[data$teacher == "B"]
boxplot(teacherA,teacherB)
#+END_SRC

#+RESULTS:
[[file:teacher.png]]

*** Grades
#+BEGIN_SRC R :session :exports both :results output
data$grade <- trunc(((data$TOTAL_score/143)*100)/10)
str(data$grade)
#+END_SRC

#+RESULTS:
:  num [1:130] 6 6 4 7 7 6 5 3 7 6 ...

*** How many students passed?
#+BEGIN_SRC R :session :exports both
table(data$grade >= 6)
#+END_SRC

#+RESULTS:
| FALSE | 86 |
| TRUE  | 44 |

*** Checking for normality

#+BEGIN_SRC R :session :file normality.png :results value graphics :exports both
x <- data$grade
h<-hist(x, breaks=10, col="red", xlab="Grade", main="Histogram with normal curve of grades")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2) 
#+END_SRC

#+RESULTS:
[[file:normality.png]]

*** Zscores
#+BEGIN_SRC R :session :exports both
zgrades <- scale(data$grade,center=TRUE, scale=TRUE)
round(zgrades[c(11,33,44,55)],2)
#+END_SRC

#+RESULTS:
|   0.8 |
| -1.58 |
|  1.99 |
|   0.8 |

*** Impressions about teacher gorup
It seems to me that teacher A is a better one.
*** The Null hypothesis
/There is no difference between the two groups./
*** Defining the variables
=teacher= is the independent variable.

*** Running the test
The default R's function assumes that there is non equal variance between the two groups. So we first check if that's the case, and in case the variance is equal, then we pass an additional argument to the function.

#+BEGIN_SRC R :session :exports both
var(teacherA)
#+END_SRC

#+RESULTS:
: 481.871186440678

#+BEGIN_SRC R :session :exports both
var(teacherB)
#+END_SRC

#+RESULTS:
: 554.467339544513


Well, at this point I don't know if this difference can be considered large enough to justify the use of the Welsh test. I'll run both.
#+BEGIN_SRC R :session :exports both
t.test(teacherA,teacherB, var.equal=TRUE)$p.value
#+END_SRC

#+RESULTS:
: 0.0250141709914793

#+BEGIN_SRC R :session :exports both
t.test(teacherA,teacherB)$p.value
#+END_SRC


#+RESULTS:
: 0.02426194067448

It is safe to reject the null hypothesis.

** Part B
#+BEGIN_SRC R :session :exports both :results output
data <- read.csv("./data/p3b.csv",na="")
colnames(data) <- c("partecipant","motivation","score")
str(data)
#+END_SRC

#+RESULTS:
: 'data.frame':	424 obs. of  3 variables:
:  $ partecipant: int  1 2 3 4 5 6 7 8 9 10 ...
:  $ motivation : Factor w/ 2 levels "High","Low": 2 2 1 2 2 2 2 1 1 1 ...
:  $ score      : int  22 28 28 26 18 31 22 25 20 25 ...

Let's define Motivation as factor.
#+BEGIN_SRC R :session :exports both :results output
data$motivation <- as.factor(data$motivation)
str(data$motivation)
#+END_SRC

#+RESULTS:
:  Factor w/ 2 levels "High","Low": 2 2 1 2 2 2 2 1 1 1 ...

Ok, now we group the scores by motivation level.
#+BEGIN_SRC R :session :exports both
bymotivation <- aggregate(data$score, by=list(data$motivation), FUN=mean)
#+END_SRC

#+RESULTS:
| High | 23.8842592592593 |
| Low  | 22.8653846153846 |

Good. There is a difference. Now we have to understand if this difference is significative or not.

#+BEGIN_SRC R :session :exports both :results output
low <- data$score[data$motivation == "Low"]
high <- data$score[data$motivation == "High"]
t.test(low,high)
#+END_SRC

#+RESULTS:
#+begin_example

	Welch Two Sample t-test

data:  low and high
t = -2.0046, df = 421.24, p-value = 0.04565
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.01795621 -0.01979307
sample estimates:
mean of x mean of y 
 22.86538  23.88426
#+end_example

#+BEGIN_SRC R :session :exports both
round(t.test(low,high)$p.value,digits=3)
#+END_SRC

#+RESULTS:
: 0.046

Yes, with this /p/ value the difference can be considered significative.
* Practical 4

Inductive statistics

** Applying the t-test

A researcher wants to find out whether boys or girls are more intelligent. Eleven girls and eight boys (randomly selected) participated in an experiment in which scores were involved ranging 1-20 (interval).

| Girls | Boys |
|-------+------|
|    17 |   16 |
|    16 |   15 |
|    14 |   13 |
|    19 |   19 |
|    18 |   15 |
|    17 |   14 |
|    16 |   13 |
|    15 |   12 |
|    16 |      |
|    15 |      |
|    19 |      |

We begin by building the dataframe.

#+BEGIN_SRC R :session :exports both
partecipant <- seq(1,19)
score <- c(17,16,16,15,14,13,19,19,18,15,17,14,16,13,15,12,16,15,19)
gender <- c(1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,1,1)
gender <- as.factor(gender)
levels(gender) <- c("F", "M")
df = data.frame(partecipant,gender,score)
str(df)
#+END_SRC

#+RESULTS:

Here we load some libraries that we are going to use later on. The first one is a plotting library, while the second contains skewness and kurtosis functions. The car packages contains Levene's test.
#+BEGIN_SRC R :session
library(psych)
library(ggplot2)
library(moments)
library(lawstat)
#+END_SRC

#+RESULTS:
| psych     |
| car       |
| moments   |
| ggplot2   |
| ltm       |
| polycor   |
| msm       |
| MASS      |
| stats     |
| graphics  |
| grDevices |
| utils     |
| datasets  |
| methods   |
| base      |

*** What are the dependent and independent variables?
Gender is the independent variable and the score is the dependent one
*** What kind of measures (nominal, ordinal or interval / scale) are used for the variables?
Gender is a nominal, while score is a scale variable.  
*** How many levels does the independent variable have?
Two, =boys= and =girls=. For readability in the output I have renamed these to =M= and =F= respectively.
*** Formulate the statistical hypothesis
- Null: there is no difference in the two groups
- H1: there is a difference: boys do better than girls
- H2: there is a difference: girls do better than boys
*** Select an alpha level suitable for this study  
0.5
*** Which statistical test could be used ?
The t-test. But we have first to check for the normality of the distribution and the homogenity.
  
*** Enter the data
/Tip/: carefully consider this step – the two columns (Girls and Boys) in the data are not necessarily the variable columns. Remember that the columns in the dataset represent variables, not levels of variables!
#+BEGIN_SRC R :session :exports both
head(df)
#+END_SRC

#+RESULTS:
| 1 | F | 17 |
| 2 | M | 16 |
| 3 | F | 16 |
| 4 | M | 15 |
| 5 | F | 14 |
| 6 | M | 13 |

*** Provide the following descriptive statistics for both groups: means, range, minimum, maximum, standard deviations.
#+BEGIN_SRC R :session :exports both :results output
f <- df$score[df$gender == "F"]
m <- df$score[df$gender == "M"]
summary(m)
summary(f)
mean(m)
mean(f)
range(m)
range(f)
sd(m)
sd(f)
#+END_SRC

#+RESULTS:
#+begin_example
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  12.00   13.00   14.50   14.62   15.25   19.00
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  14.00   15.50   16.00   16.55   17.50   19.00
[1] 14.625
[1] 16.54545
[1] 12 19
[1] 14 19
[1] 2.199838
[1] 1.634848
#+end_example

*** What are your first impressions about the difference between the boys and the girls?
Let's take a look.

#+BEGIN_SRC R :session :file boysgirls.png :results value graphics :exports both
boxplot(f,m,names=c("Girls","Boys"))
#+END_SRC

#+RESULTS:
[[file:boysgirls.png]]

It seems that girls score better than the boys.
*** Create a box plot to visualise the results. 
Done in the previous section.
*** Test the statistical significance of this experiment

Find out which group has a distribution that most resembles the normal distribution.

  What do the values of skewness and kurtosis represent again ?  How can they help you in determining whether a dataset resembles a normal distribution?  Check the “How To Check Assumptions NEW” on Nestor as well.

In our dataset we have 19 observations. So, we are going to run the Shapiro-Wilk test.

#+BEGIN_SRC R :session :exports both :results output
shapiro.test(m)
shapiro.test(f)
#+END_SRC

#+RESULTS:
#+begin_example

	Shapiro-Wilk normality test

data:  m
W = 0.9228, p-value = 0.453

	Shapiro-Wilk normality test

data:  f
W = 0.94182, p-value = 0.5422
#+end_example

Both groups resembles a normal distribution. We now take a look at skewness and kurtosis.

#+BEGIN_SRC R :session :exports both :results output
skewness(m)
kurtosis(m)
skewness(f)
kurtosis(f)
#+END_SRC

#+RESULTS:
: [1] 0.8540259
: [1] 3.008633
: [1] 0.203529
: [1] 2.014369

The =boys= group presents higher values for both skewness and kurtosis when compaird to =girls=. So =girls= has a more normal distribution.

**** Do  the Independent samples t-test.
Why do you have to use this test rather than the one sample t-test or the paired samples t-test ?

#+BEGIN_SRC R :session :exports both :results output
t.test(m,f)
#+END_SRC

#+RESULTS:
#+begin_example

	Welch Two Sample t-test

data:  m and f
t = -2.0856, df = 12.357, p-value = 0.05838
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -3.92030824  0.07939915
sample estimates:
mean of x mean of y 
 14.62500  16.54545
#+end_example

**** Carefully study the output
**** Leven's test
Taking Levene’s test into account, what is the value of “t”?  Which degrees of freedom are applied to this test?  What is the level of significance of these samples ?  Compare this to the alpha level you set in e) above.  Can you reject H 0 ?
#+RESULTS:

#+BEGIN_SRC R :session :exports both :results output
levene.test(df$score, df$gender, location="median")
#+END_SRC

#+RESULTS:
: 
: 	modified robust Brown-Forsythe Levene-type test based on the absolute
: 	deviations from the median
: 
: data:  df$score
: Test Statistic = 0.38995, p-value = 0.5406

Since the p-value of the Levene's test is greater than 0.05, I would say that the test is not signicant and so the two groups should have a similar variance. But from the plot it doesn't seem so. Indeed if we compare the two variances we can see that one is more than twice the other. I suspect there is something wrong with the test.

#+BEGIN_SRC R :session :exports both :results output
var(m)
var(f)
#+END_SRC

#+RESULTS:
: [1] 4.839286
: [1] 2.672727


#+BEGIN_QUOTE
On average, the girls showed a higher level of intelligence (M=14.63, SE= ... )  than the boys(M=14.63. , SE= ... ). This difference was not significant t(df=12.36,t=-2.09, p > 0.05).
#+END_QUOTE

** What can you say about the meaningfulness of this outcome?
Is there any additional information you’d like to have about this study ?

Not much. I would like to have more data
** Consider the following data
8 students have participated in a reading test and a listening comprehension test.  Reading ability and listening comprehension are operationalised by the variables R and L respectively. Both variables are measured on an interval scale. The results have been summarised in the table below. Build a dataframe.

| Student |   R |  L |
|---------+-----+----|
|       1 |  20 | 65 |
|       2 |  40 | 69 |
|       3 |  60 | 73 |
|       4 |  80 | 77 |
|       5 | 100 | 80 |
|       6 | 120 | 84 |
|       7 | 140 | 89 |
|       8 | 160 | 95 |
#+BEGIN_SRC R :session
partecipant <- seq(1,8)
r <- c(20,40,60,80,100,120,140,160)
l <- c(65,69,73,77,80,84,89,95)
df = data.frame(partecipant,r,l)
#+END_SRC

#+RESULTS:
| 1 |  20 | 65 |
| 2 |  40 | 69 |
| 3 |  60 | 73 |
| 4 |  80 | 77 |
| 5 | 100 | 80 |
| 6 | 120 | 84 |
| 7 | 140 | 89 |
| 8 | 160 | 95 |

*** What would be H_0 if we want to test the relationship between reading and listening comprehension?
Reading and listening do not interfere.  
*** Make a plot of the results.

#+BEGIN_SRC R :session :file correlation.png :results value graphics :exports both
plot(df$r,df$l,xlab="Reading",ylab="Listening")
#+END_SRC

*** At face value, do you think Reading and Listening , as plotted in the graph, are related?
Yes
*** We want to know if we can conclude that reading skills and listening comprehension are significantly related.
To determine this, you will have to calc ulate a Pearson r (or r xy ). Make sure the computer calculates the Pearson correlation for a two-tailed test.  What is the value of r xy ? Is this a strong correlation? What is the chance of incorrectly rejecting your H 0 ? What do you decide?

#+BEGIN_SRC R :session :exports both :results output
cor(df$r,df$l,method="pearson")
#+END_SRC

#+RESULTS:
: 0.996229128491916

#+BEGIN_SRC R :session :exports both :results output
t.test(df$r,df$l)
#+END_SRC

#+RESULTS:
#+begin_example

	Welch Two Sample t-test

data:  df$r and df$l
t = 0.62193, df = 7.5972, p-value = 0.5522
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -30.16518  52.16518
sample estimates:
mean of x mean of y 
       90        79
#+end_example

*** Report

#+BEGIN_QUOTE
A correlation analysis showed that Reading Skills and Listening Skills were not significantly related (r =0.99, p > 0.05
#+END_QUOTE
*** Cronbach's Alpha

...we shortly discussed reliability, and that Cronbach’s Alpha was a good measure to check for reliability of a test. The teachers from the data in Practical 3A are interested in the reliability of their exam. They have decided to use Cronbach’s Alpha to check this
1) Open the data for Prac3A t o check the reliability of a 17-item phonetics test
2) Decide whether the test is reliable by going to Analyze > Scale > Reliability Analysis.  Put all the Qu estions in the Items (and not the Total and the Grade), and choose Alpha next to Model . Click OK. The Output will give you a correlation coefficient.  Do you think this is a reliable test?
3) Now we will check the individual items. Go to Analyze > Scale > Reliability Analysis.  Click on Statistics. Check Inter-Item Correlations and Descriptives for Scale if item deleted. Click OK. The output will give you the correlations between items and will give you all the Cronbach’s Alpha values without a particular item. With the deletion of which item do you get the highest reliabil

#+BEGIN_SRC R :session :exports both :results output
alpha(data,delete=TRUE,check.keys=TRUE)
#+END_SRC

#+RESULTS:
#+begin_example

Reliability analysis   
Call: alpha(x = data, check.keys = TRUE, delete = TRUE)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd
      0.28      0.82    0.85       0.2 4.6 0.064  7.7 2.6

 lower alpha upper     95% confidence boundaries
0.15 0.28 0.41 

 Reliability if an item is dropped:
          raw_alpha std.alpha G6(smc) average_r S/N alpha se
Student.-      0.80      0.83    0.85      0.22 4.9    0.021
Q1             0.18      0.80    0.83      0.19 3.9    0.064
Q2             0.27      0.81    0.83      0.20 4.1    0.064
Q3             0.26      0.80    0.83      0.19 3.9    0.064
Q4             0.26      0.80    0.83      0.19 4.1    0.064
Q5             0.26      0.80    0.83      0.19 4.0    0.064
Q6             0.27      0.80    0.83      0.19 4.0    0.064
Q7             0.27      0.81    0.84      0.20 4.2    0.064
Q8             0.27      0.81    0.84      0.20 4.2    0.060
Q9             0.27      0.82    0.85      0.21 4.6    0.064
Q10            0.27      0.82    0.84      0.21 4.4    0.064
Q11            0.28      0.82    0.85      0.22 4.7    0.064
Q12            0.28      0.83    0.85      0.22 4.7    0.063
Q13            0.26      0.81    0.84      0.20 4.4    0.065
Q14            0.29      0.83    0.86      0.23 5.0    0.064
Q15            0.28      0.82    0.85      0.22 4.7    0.063
Q16            0.27      0.80    0.83      0.19 4.0    0.063
Q17            0.20      0.80    0.83      0.19 3.9    0.065

 Item statistics 
            n  raw.r std.r r.cor r.drop mean   sd
Student.- 130  0.865  0.16 0.078  0.089 64.5 37.7
Q1        130  0.570  0.74 0.749  0.464 10.6  6.4
Q2        130  0.320  0.60 0.579  0.299  4.0  1.1
Q3        130  0.392  0.73 0.734  0.358  4.3  1.8
Q4        130  0.355  0.63 0.617  0.315  6.6  2.1
Q5        130  0.328  0.70 0.703  0.293  3.9  1.8
Q6        130  0.394  0.66 0.647  0.373  2.1  1.2
Q7        130  0.256  0.57 0.545  0.223  2.1  1.6
Q8        130  0.213  0.54 0.509  0.125 13.2  4.1
Q9        130  0.240  0.34 0.266  0.188  2.8  2.5
Q10       130  0.234  0.43 0.368  0.182  2.6  2.5
Q11       130  0.104  0.29 0.209  0.055  3.6  2.3
Q12       130  0.072  0.27 0.180  0.024  1.3  2.2
Q13       130  0.347  0.48 0.427  0.298  2.3  2.5
Q14       130 -0.014  0.12 0.022 -0.059  1.0  2.0
Q15       130  0.088  0.30 0.219  0.042  1.1  2.1
Q16       130  0.242  0.66 0.654  0.215  3.5  1.3
Q17       130  0.573  0.73 0.729  0.498  8.3  4.7
#+end_example

I confess the alpha is not completley clear to me. Here I am submitting the results as it is.

** Testing for normality
Apply Ks test.

Please note: if you want to test for normality in an experiment with more than one group, you’ll have to run separate analyses for the each group. It’s important the distribution of each group is normal, rather than the distribution of the scores of the two groups taken together.

#+BEGIN_SRC R :session :exports both :results output
data <- read.csv("./data/p3a.csv",na="",header=TRUE)
data$totalscore <- rowSums(data[,4:20])
ks.test((data$totalscore),"pnorm")
#+END_SRC

#+RESULTS:
#+begin_example

	One-sample Kolmogorov-Smirnov test

data:  (data$totalscore)
D = 1, p-value < 2.2e-16
alternative hypothesis: two-sided

Warning message:
In ks.test((data$totalscore), "pnorm") :
  ties should not be present for the Kolmogorov-Smirnov test
#+end_example

It seems that there are repeated values in the =TOTALscore= variable. In fact, shouldn't the KS test be applied to continous distributions only ? In the next section I run it on unique elements of TOTALscore and it correclty reports no warning. Maybe is SPSS doing this automatically?

#+BEGIN_SRC R :session :exports both :results output
data <- read.csv("./data/p3a.csv",na="",header=TRUE)
data$totalscore <- rowSums(data[,4:20])
ks.test(unique(data$totalscore),"pnorm")
#+END_SRC

#+RESULTS:
: 
: 	One-sample Kolmogorov-Smirnov test
: 
: data:  unique(data$totalscore)
: D = 1, p-value = 2.22e-16
: alternative hypothesis: two-sided

* Notes								   :noexport:
** Always check for normality before running t-test
** Equality of variance 
A rule of thumb with equality of variance is that the largest SD of your groups should not be more than twice the smallest SD of your groups.
** Reporting guidelines
http://my.ilstu.edu/~jhkahn/apastats.html
